{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>STEP 1: Load data and prepare for Torch</strong>\n",
    "#### <strong>AT&T人臉圖示範</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "df = pd.read_csv('/Users/andrewhsu/Library/Mobile Documents/com~apple~CloudDocs/VsCode_python/Excel_file/face_data.csv')\n",
    "n_persons = df['target'].nunique() \n",
    "X = np.array(df.drop('target', axis=1)) # 400 x 4096\n",
    "y = np.array(df['target'])\n",
    " \n",
    "# 以numpy的矩陣形式輸入，非dataframe\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # deafult test_size=0.25\n",
    " \n",
    "# prepare data for PyTorch Tensor(一種矩陣資料型態)\n",
    "X_train = torch.from_numpy(X_train).float() # convert to float tensor\n",
    "y_train = torch.from_numpy(y_train).float() # \n",
    "train_dataset = TensorDataset(X_train, y_train) # create your datset # 再次把X、Y合起來\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "test_dataset = TensorDataset(X_test, y_test) # create your datset\n",
    " \n",
    "# create dataloader for PyTorch\n",
    "# 一次讀取的資料量，不會整組拿去續練，而是一次拿一小部分\n",
    "batch_size = 64 # 32, 64, 128, 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # convert to dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(X_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>STEP 2 :Set up NN Model</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=40, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "# select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\" # run faster than cuda in some cases\n",
    "print(\"Using {} device\".format(device))\n",
    " \n",
    "# Create a neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64*64, 512), # image length 64x64=4096,  fully connected layer\n",
    "            nn.ReLU(), # try to take ReLU out to see what happen\n",
    "            nn.Linear(512, 128), # second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 40) # 40 classes,  fully connected layer\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    # Specify how data will pass through this model\n",
    "    def forward(self, x):\n",
    "        # out = self.mlp(x) \n",
    " \n",
    "        # Apply softmax to x here~\n",
    "        x = self.mlp(x)\n",
    "        # 以下羅吉斯迴歸\n",
    "        out = F.log_softmax(x, dim=1) # it’s faster and has better numerical propertie than softmax\n",
    "        # out = F.softmax(x, dim=1)\n",
    "        return out\n",
    " \n",
    " \n",
    "# define model, optimizer, loss function\n",
    "model = MLP().to(device) # start an instance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # default lreaning rate=1e-3 #定義演算法\n",
    "loss_fun = nn.CrossEntropyLoss() # define loss function #定義損失函數\n",
    " \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40])\n",
      "tensor([[0.0257, 0.0222, 0.0231,  ..., 0.0209, 0.0186, 0.0311],\n",
      "        [0.0203, 0.0202, 0.0231,  ..., 0.0175, 0.0239, 0.0217],\n",
      "        [0.0297, 0.0179, 0.0184,  ..., 0.0275, 0.0180, 0.0241],\n",
      "        ...,\n",
      "        [0.0180, 0.0255, 0.0191,  ..., 0.0166, 0.0163, 0.0235],\n",
      "        [0.0229, 0.0205, 0.0208,  ..., 0.0155, 0.0166, 0.0251],\n",
      "        [0.0225, 0.0254, 0.0204,  ..., 0.0146, 0.0207, 0.0240]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([31, 30,  9,  9, 28, 18,  6,  3,  9,  5,  5,  7, 36, 22, 28,  3, 32,  3,\n",
      "        22, 32,  0,  9, 16, 15, 39,  5,  0,  9, 28, 28, 12, 22,  9, 28,  3, 18,\n",
      "        22, 32, 18, 18,  9,  9, 30, 30,  0,  6, 20, 36,  8, 22, 28,  6,  9,  6,\n",
      "        22, 22, 18,  6, 28, 30, 36, 22,  9, 21,  8, 22, 16, 36,  8, 17,  9, 32,\n",
      "         6, 28,  7,  8,  6,  3, 33,  9,  6,  5, 28,  9, 25, 36, 36, 36,  3, 16,\n",
      "        22, 31,  6, 28, 36, 22, 32, 28, 22, 31])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(100, 64 * 64) \n",
    "m1 = nn.Linear(64*64, 512)\n",
    "output = m1(input)\n",
    "print(output.size())\n",
    "output = F.relu(output)\n",
    "print(output.size())\n",
    "m2 = nn.Linear(512, 40)\n",
    "output = m2(output)\n",
    "print(output.size())\n",
    "output = F.log_softmax(output, dim=1)\n",
    "print(output.size())\n",
    "\n",
    "output = F.softmax(output, dim=1)\n",
    "print(output.size())\n",
    "print(output)\n",
    "print(output.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>STEP 3: Start training</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   3.70\n",
      "\tEpoch 0 | Batch 4 | Loss   3.74\n",
      "Epoch 0 | Loss   3.81 | train accuracy 0.0393\n",
      "\tEpoch 1 | Batch 0 | Loss   3.73\n",
      "\tEpoch 1 | Batch 4 | Loss   3.86\n",
      "Epoch 1 | Loss   3.72 | train accuracy 0.0250\n",
      "\tEpoch 2 | Batch 0 | Loss   3.65\n",
      "\tEpoch 2 | Batch 4 | Loss   3.63\n",
      "Epoch 2 | Loss   3.66 | train accuracy 0.0393\n",
      "\tEpoch 3 | Batch 0 | Loss   3.63\n",
      "\tEpoch 3 | Batch 4 | Loss   3.49\n",
      "Epoch 3 | Loss   3.62 | train accuracy 0.0607\n",
      "\tEpoch 4 | Batch 0 | Loss   3.60\n",
      "\tEpoch 4 | Batch 4 | Loss   3.65\n",
      "Epoch 4 | Loss   3.62 | train accuracy 0.0964\n",
      "\tEpoch 5 | Batch 0 | Loss   3.58\n",
      "\tEpoch 5 | Batch 4 | Loss   3.44\n",
      "Epoch 5 | Loss   3.56 | train accuracy 0.0643\n",
      "\tEpoch 6 | Batch 0 | Loss   3.52\n",
      "\tEpoch 6 | Batch 4 | Loss   3.66\n",
      "Epoch 6 | Loss   3.56 | train accuracy 0.0964\n",
      "\tEpoch 7 | Batch 0 | Loss   3.49\n",
      "\tEpoch 7 | Batch 4 | Loss   3.46\n",
      "Epoch 7 | Loss   3.50 | train accuracy 0.0786\n",
      "\tEpoch 8 | Batch 0 | Loss   3.46\n",
      "\tEpoch 8 | Batch 4 | Loss   3.40\n",
      "Epoch 8 | Loss   3.44 | train accuracy 0.1214\n",
      "\tEpoch 9 | Batch 0 | Loss   3.32\n",
      "\tEpoch 9 | Batch 4 | Loss   3.33\n",
      "Epoch 9 | Loss   3.37 | train accuracy 0.1214\n",
      "\tEpoch 10 | Batch 0 | Loss   3.29\n",
      "\tEpoch 10 | Batch 4 | Loss   3.45\n",
      "Epoch 10 | Loss   3.34 | train accuracy 0.1893\n",
      "\tEpoch 11 | Batch 0 | Loss   3.17\n",
      "\tEpoch 11 | Batch 4 | Loss   3.30\n",
      "Epoch 11 | Loss   3.23 | train accuracy 0.1643\n",
      "\tEpoch 12 | Batch 0 | Loss   3.14\n",
      "\tEpoch 12 | Batch 4 | Loss   3.17\n",
      "Epoch 12 | Loss   3.15 | train accuracy 0.1893\n",
      "\tEpoch 13 | Batch 0 | Loss   3.14\n",
      "\tEpoch 13 | Batch 4 | Loss   3.07\n",
      "Epoch 13 | Loss   3.03 | train accuracy 0.2357\n",
      "\tEpoch 14 | Batch 0 | Loss   2.91\n",
      "\tEpoch 14 | Batch 4 | Loss   2.97\n",
      "Epoch 14 | Loss   2.93 | train accuracy 0.2286\n",
      "\tEpoch 15 | Batch 0 | Loss   2.84\n",
      "\tEpoch 15 | Batch 4 | Loss   2.70\n",
      "Epoch 15 | Loss   2.80 | train accuracy 0.3036\n",
      "\tEpoch 16 | Batch 0 | Loss   2.47\n",
      "\tEpoch 16 | Batch 4 | Loss   2.41\n",
      "Epoch 16 | Loss   2.68 | train accuracy 0.2429\n",
      "\tEpoch 17 | Batch 0 | Loss   2.44\n",
      "\tEpoch 17 | Batch 4 | Loss   2.56\n",
      "Epoch 17 | Loss   2.62 | train accuracy 0.2929\n",
      "\tEpoch 18 | Batch 0 | Loss   2.48\n",
      "\tEpoch 18 | Batch 4 | Loss   2.73\n",
      "Epoch 18 | Loss   2.49 | train accuracy 0.4036\n",
      "\tEpoch 19 | Batch 0 | Loss   2.42\n",
      "\tEpoch 19 | Batch 4 | Loss   2.26\n",
      "Epoch 19 | Loss   2.38 | train accuracy 0.3929\n",
      "\tEpoch 20 | Batch 0 | Loss   2.21\n",
      "\tEpoch 20 | Batch 4 | Loss   2.10\n",
      "Epoch 20 | Loss   2.24 | train accuracy 0.4679\n",
      "\tEpoch 21 | Batch 0 | Loss   2.30\n",
      "\tEpoch 21 | Batch 4 | Loss   2.19\n",
      "Epoch 21 | Loss   2.18 | train accuracy 0.4571\n",
      "\tEpoch 22 | Batch 0 | Loss   2.06\n",
      "\tEpoch 22 | Batch 4 | Loss   2.18\n",
      "Epoch 22 | Loss   2.08 | train accuracy 0.5179\n",
      "\tEpoch 23 | Batch 0 | Loss   2.05\n",
      "\tEpoch 23 | Batch 4 | Loss   1.67\n",
      "Epoch 23 | Loss   1.96 | train accuracy 0.4643\n",
      "\tEpoch 24 | Batch 0 | Loss   1.92\n",
      "\tEpoch 24 | Batch 4 | Loss   1.86\n",
      "Epoch 24 | Loss   1.92 | train accuracy 0.5286\n",
      "\tEpoch 25 | Batch 0 | Loss   1.82\n",
      "\tEpoch 25 | Batch 4 | Loss   1.66\n",
      "Epoch 25 | Loss   1.81 | train accuracy 0.5464\n",
      "\tEpoch 26 | Batch 0 | Loss   1.81\n",
      "\tEpoch 26 | Batch 4 | Loss   1.45\n",
      "Epoch 26 | Loss   1.74 | train accuracy 0.5750\n",
      "\tEpoch 27 | Batch 0 | Loss   1.79\n",
      "\tEpoch 27 | Batch 4 | Loss   1.90\n",
      "Epoch 27 | Loss   1.77 | train accuracy 0.5250\n",
      "\tEpoch 28 | Batch 0 | Loss   1.49\n",
      "\tEpoch 28 | Batch 4 | Loss   1.56\n",
      "Epoch 28 | Loss   1.61 | train accuracy 0.6250\n",
      "\tEpoch 29 | Batch 0 | Loss   1.67\n",
      "\tEpoch 29 | Batch 4 | Loss   1.59\n",
      "Epoch 29 | Loss   1.63 | train accuracy 0.5500\n",
      "\tEpoch 30 | Batch 0 | Loss   1.57\n",
      "\tEpoch 30 | Batch 4 | Loss   1.42\n",
      "Epoch 30 | Loss   1.47 | train accuracy 0.7107\n",
      "\tEpoch 31 | Batch 0 | Loss   1.35\n",
      "\tEpoch 31 | Batch 4 | Loss   1.56\n",
      "Epoch 31 | Loss   1.46 | train accuracy 0.6821\n",
      "\tEpoch 32 | Batch 0 | Loss   1.30\n",
      "\tEpoch 32 | Batch 4 | Loss   1.34\n",
      "Epoch 32 | Loss   1.42 | train accuracy 0.6750\n",
      "\tEpoch 33 | Batch 0 | Loss   1.39\n",
      "\tEpoch 33 | Batch 4 | Loss   1.22\n",
      "Epoch 33 | Loss   1.31 | train accuracy 0.7071\n",
      "\tEpoch 34 | Batch 0 | Loss   1.37\n",
      "\tEpoch 34 | Batch 4 | Loss   1.17\n",
      "Epoch 34 | Loss   1.23 | train accuracy 0.7536\n",
      "\tEpoch 35 | Batch 0 | Loss   1.21\n",
      "\tEpoch 35 | Batch 4 | Loss   1.13\n",
      "Epoch 35 | Loss   1.18 | train accuracy 0.7536\n",
      "\tEpoch 36 | Batch 0 | Loss   1.27\n",
      "\tEpoch 36 | Batch 4 | Loss   0.97\n",
      "Epoch 36 | Loss   1.13 | train accuracy 0.7179\n",
      "\tEpoch 37 | Batch 0 | Loss   1.08\n",
      "\tEpoch 37 | Batch 4 | Loss   1.26\n",
      "Epoch 37 | Loss   1.18 | train accuracy 0.7214\n",
      "\tEpoch 38 | Batch 0 | Loss   1.05\n",
      "\tEpoch 38 | Batch 4 | Loss   1.24\n",
      "Epoch 38 | Loss   1.08 | train accuracy 0.7714\n",
      "\tEpoch 39 | Batch 0 | Loss   1.06\n",
      "\tEpoch 39 | Batch 4 | Loss   0.90\n",
      "Epoch 39 | Loss   0.99 | train accuracy 0.8286\n",
      "\tEpoch 40 | Batch 0 | Loss   1.15\n",
      "\tEpoch 40 | Batch 4 | Loss   0.84\n",
      "Epoch 40 | Loss   0.97 | train accuracy 0.8000\n",
      "\tEpoch 41 | Batch 0 | Loss   0.91\n",
      "\tEpoch 41 | Batch 4 | Loss   0.98\n",
      "Epoch 41 | Loss   0.91 | train accuracy 0.8179\n",
      "\tEpoch 42 | Batch 0 | Loss   0.95\n",
      "\tEpoch 42 | Batch 4 | Loss   0.79\n",
      "Epoch 42 | Loss   0.86 | train accuracy 0.8357\n",
      "\tEpoch 43 | Batch 0 | Loss   0.87\n",
      "\tEpoch 43 | Batch 4 | Loss   0.64\n",
      "Epoch 43 | Loss   0.79 | train accuracy 0.8464\n",
      "\tEpoch 44 | Batch 0 | Loss   0.85\n",
      "\tEpoch 44 | Batch 4 | Loss   0.82\n",
      "Epoch 44 | Loss   0.79 | train accuracy 0.8536\n",
      "\tEpoch 45 | Batch 0 | Loss   0.62\n",
      "\tEpoch 45 | Batch 4 | Loss   0.90\n",
      "Epoch 45 | Loss   0.77 | train accuracy 0.8679\n",
      "\tEpoch 46 | Batch 0 | Loss   0.73\n",
      "\tEpoch 46 | Batch 4 | Loss   0.86\n",
      "Epoch 46 | Loss   0.73 | train accuracy 0.8929\n",
      "\tEpoch 47 | Batch 0 | Loss   0.63\n",
      "\tEpoch 47 | Batch 4 | Loss   0.59\n",
      "Epoch 47 | Loss   0.66 | train accuracy 0.9071\n",
      "\tEpoch 48 | Batch 0 | Loss   0.57\n",
      "\tEpoch 48 | Batch 4 | Loss   0.71\n",
      "Epoch 48 | Loss   0.65 | train accuracy 0.8786\n",
      "\tEpoch 49 | Batch 0 | Loss   0.62\n",
      "\tEpoch 49 | Batch 4 | Loss   0.74\n",
      "Epoch 49 | Loss   0.62 | train accuracy 0.9214\n",
      "Finished ... Loss  0.6156 | train accuracy 0.9214\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "epochs = 50 # Repeat the whole dataset epochs times\n",
    "model.train() # Sets the module in training mode. The training model allow the parameters to be updated during backpropagation.\n",
    "for epoch in range(epochs):\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "    # for batch_num, input_data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "         \n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    " \n",
    "        # perform training based on the backpropagation\n",
    "        y_pre = model(x) # predict y\n",
    "        loss = loss_fun(y_pre, y.long()) # the loss function nn.CrossEntropyLoss()\n",
    "        losses.append(loss.item())\n",
    " \n",
    "        optimizer.zero_grad() # Zeros the gradients accumulated from the previous batch/step of the model\n",
    "        loss.backward() # Performs backpropagation and calculates the gradients\n",
    "        optimizer.step() # Updates the weights in our neural network based on the results of backpropagation\n",
    "         \n",
    "        # Record the training accuracy for each batch\n",
    "        trainAcc += (y_pre.argmax(dim=1) == y).sum().item() # comparison\n",
    "        samples += y.size(0)\n",
    "        if batch_num % 4 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f | train accuracy %.4f' % (epoch, sum(losses)/len(losses), trainAcc/samples))\n",
    " \n",
    "print('Finished ... Loss %7.4f | train accuracy %.4f' % (sum(losses)/len(losses), trainAcc/samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>Testing (1) : Compute test accuracy by batch</strong> \n",
    "完成訓練後的神經網路模組，必須透過測試資料進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.758\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    "with torch.no_grad():\n",
    "    for x, y_truth in test_loader:\n",
    "        x = x.to(device).float()\n",
    "        y_truth = y_truth.to(device)\n",
    "        y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "        testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "        samples += y_truth.size(0)\n",
    " \n",
    "    print('Test Accuracy:{:.3f}'.format(testAcc/samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>Testing (2): Compute the test accuracy and record the result for each test data</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.758\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    " \n",
    "# use eval() in conjunction with a torch.no_grad() context, \n",
    "# meaning that gradient computation is turned off in evaluation mode\n",
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    " \n",
    "with open('mlp_att.csv', 'w') as f:\n",
    "    fieldnames = ['ImageId', 'Label', 'Ground_Truth']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    image_id = 1\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for x, y_truth in test_loader:\n",
    "            x = x.to(device).float()\n",
    "            y_truth = y_truth.to(device).long()\n",
    "            yIdx = 0\n",
    "            y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "            testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "            samples += y_truth.size(0)\n",
    "            for y in y_pre:\n",
    "                writer.writerow({fieldnames[0]: image_id,fieldnames[1]: y.item(), fieldnames[2]: y_truth[yIdx].item()})\n",
    "                image_id += 1\n",
    "                yIdx += 1\n",
    " \n",
    "        print('Test Accuracy:{:.3f}'.format(testAcc/samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
